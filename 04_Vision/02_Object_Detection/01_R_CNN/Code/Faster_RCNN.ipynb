{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP18qF021sMxMyVAnZygl6i"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"v1rfyKjjgW2W","executionInfo":{"status":"ok","timestamp":1691657695560,"user_tz":-210,"elapsed":56425,"user":{"displayName":"Mahmoud Alipour","userId":"01162435858311436483"}},"outputId":"daf1972f-ee97-4511-d635-64be33c41886"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6Dfh0qUAoTAo","executionInfo":{"status":"ok","timestamp":1691657695561,"user_tz":-210,"elapsed":22,"user":{"displayName":"Mahmoud Alipour","userId":"01162435858311436483"}},"outputId":"5f25d8c2-2821-4ea1-a028-c8908a33270a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["import os\n","os.chdir('/content/drive/MyDrive/Faster_RCNN/me')\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNUJzDBGoUQ3","executionInfo":{"status":"ok","timestamp":1691657711493,"user_tz":-210,"elapsed":390,"user":{"displayName":"Mahmoud Alipour","userId":"01162435858311436483"}},"outputId":"8222e67d-4a0c-478e-f1b3-4f26371b044b"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Faster_RCNN/me\n"]}]},{"cell_type":"code","source":["#Install import-ipynb library from the command prompt\n","!pip install import-ipynb\n","\n","#Import it from your notebook\n","import import_ipynb\n","\n","#Import your BBB.ipynb notebook as if it was BBB.py file\n","from utils import generate_anchors, draw_anchors, bbox_overlaps, bbox_transform,\\\n","                    loss_cls, smoothL1, parse_label, unmap"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dKX_i7t6oXQR","executionInfo":{"status":"ok","timestamp":1691657723973,"user_tz":-210,"elapsed":11030,"user":{"displayName":"Mahmoud Alipour","userId":"01162435858311436483"}},"outputId":"09dd4acd-b980-4600-bb69-1f4d9109bff2"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting import-ipynb\n","  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n","Requirement already satisfied: IPython in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (7.34.0)\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.10/dist-packages (from import-ipynb) (5.9.2)\n","Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (67.7.2)\n","Collecting jedi>=0.16 (from IPython->import-ipynb)\n","  Downloading jedi-0.19.0-py2.py3-none-any.whl (1.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.4.2)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.7.5)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (5.7.1)\n","Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (3.0.39)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (2.14.0)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.2.0)\n","Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (0.1.6)\n","Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from IPython->import-ipynb) (4.8.0)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (2.18.0)\n","Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (4.3.3)\n","Requirement already satisfied: jupyter-core in /usr/local/lib/python3.10/dist-packages (from nbformat->import-ipynb) (5.3.1)\n","Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.3)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (23.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=2.6->nbformat->import-ipynb) (0.19.3)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.6)\n","Requirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.10/dist-packages (from jupyter-core->nbformat->import-ipynb) (3.10.0)\n","Installing collected packages: jedi, import-ipynb\n","Successfully installed import-ipynb-0.1.4 jedi-0.19.0\n","importing Jupyter notebook from utils.ipynb\n","Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GLY06SgeXqgS","executionInfo":{"status":"error","timestamp":1691657762892,"user_tz":-210,"elapsed":37252,"user":{"displayName":"Mahmoud Alipour","userId":"01162435858311436483"}},"outputId":"8f687800-6fc3-4f0c-ab4e-638526b39bce"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_1 (InputLayer)           [(None, None, None,  0           []                               \n","                                 1536)]                                                           \n","                                                                                                  \n"," input_2 (InputLayer)           [(None, 4)]          0           []                               \n","                                                                                                  \n"," input_3 (InputLayer)           [(None, 1)]          0           []                               \n","                                                                                                  \n"," ro_i_pooling (RoIPooling)      (None, 7, 7, 1536)   0           ['input_1[0][0]',                \n","                                                                  'input_2[0][0]',                \n","                                                                  'input_3[0][0]']                \n","                                                                                                  \n"," flatten (Flatten)              (None, 75264)        0           ['ro_i_pooling[0][0]']           \n","                                                                                                  \n"," fc2 (Dense)                    (None, 1024)         77071360    ['flatten[0][0]']                \n","                                                                                                  \n"," batch_normalization (BatchNorm  (None, 1024)        4096        ['fc2[0][0]']                    \n"," alization)                                                                                       \n","                                                                                                  \n"," scores2 (Dense)                (None, 200)          205000      ['batch_normalization[0][0]']    \n","                                                                                                  \n"," deltas2 (Dense)                (None, 800)          820000      ['batch_normalization[0][0]']    \n","                                                                                                  \n","==================================================================================================\n","Total params: 78,100,456\n","Trainable params: 78,098,408\n","Non-trainable params: 2,048\n","__________________________________________________________________________________________________\n","Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n","219055592/219055592 [==============================] - 3s 0us/step\n","1/1 [==============================] - 6s 6s/step\n"]},{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-3d233ced1566>\u001b[0m in \u001b[0;36m<cell line: 94>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0mnot_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpretrained_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m rpn_model = load_model('weights.hdf5',\n\u001b[0m\u001b[1;32m     95\u001b[0m             custom_objects={'loss_cls': loss_cls,'smoothL1':smoothL1})\n\u001b[1;32m     96\u001b[0m \u001b[0mnot_used\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrpn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'n02676566_6914'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/saving_api.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, safe_mode, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# Legacy case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m     return legacy_sm_saving_lib.load_model(\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_objects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/saving/legacy/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    228\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m                             raise IOError(\n\u001b[0m\u001b[1;32m    231\u001b[0m                                 \u001b[0;34mf\"No file or directory found at {filepath_str}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m                             )\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at weights.hdf5"]}],"source":["import os\n","import traceback\n","import numpy as np\n","import numpy.random as npr\n","import tensorflow as tf\n","import keras.backend as K\n","from keras.models import load_model\n","from keras.layers import Conv2D, TimeDistributed, Flatten, Dense, BatchNormalization\n","from keras.layers import Layer\n","from keras.layers import Input\n","from keras import Model\n","from keras.applications import InceptionResNetV2\n","from keras.utils import load_img, img_to_array\n","from utils import generate_anchors, draw_anchors, bbox_overlaps, bbox_transform,\\\n","                    loss_cls, smoothL1, parse_label, unmap, filter_boxes, \\\n","                    clip_boxes, py_cpu_nms, bbox_transform_inv\n","\n","\n","##################  R-CNN Model  #######################\n","# RoI Pooling layer\n","class RoIPooling(Layer):\n","    def __init__(self, size=(7, 7)):\n","        self.size = size\n","        super(RoIPooling, self).__init__()\n","\n","    def build(self, input_shape):\n","        self.shape = input_shape\n","        super(RoIPooling, self).build(input_shape)\n","\n","    def call(self, inputs, **kwargs):\n","        ind=K.reshape(inputs[2],(-1,))\n","        x = K.tf.image.crop_and_resize(inputs[0], inputs[1], ind, self.size)\n","        return x\n","\n","    def compute_output_shape(self, input_shape):\n","        a=input_shape[1][0]\n","        b=self.size[0]\n","        c=self.size[1]\n","        d=input_shape[0][3]\n","        return (a,b,c,d)\n","\n","\n","BATCH=256\n","\n","feature_map=Input(batch_shape=(None,None,None,1536))\n","rois=Input(batch_shape=(None, 4))\n","ind=Input(batch_shape=(None, 1),dtype='int32')\n","\n","p1=RoIPooling()([feature_map, rois, ind])\n","\n","flat1 = Flatten()(p1)\n","\n","\n","fc1 = Dense(\n","        units=1024,\n","        activation=\"relu\",\n","        name=\"fc2\"\n","    )(flat1)\n","fc1=BatchNormalization()(fc1)\n","output_deltas = Dense(\n","        units=4 * 200,\n","        activation=\"linear\",\n","        kernel_initializer=\"uniform\",\n","        name=\"deltas2\"\n","    )(fc1)\n","\n","output_scores = Dense(\n","        units=1 * 200,\n","        activation=\"softmax\",\n","        kernel_initializer=\"uniform\",\n","        name=\"scores2\"\n","    )(fc1)\n","\n","model=Model(inputs=[feature_map, rois, ind],outputs=[output_scores,output_deltas])\n","model.summary()\n","model.compile(optimizer='rmsprop',\n","            loss={'deltas2':smoothL1, 'scores2':'categorical_crossentropy'})\n","\n","##################  prepare batch  #######################\n","\n","FG_FRAC=.25\n","FG_THRESH=.5\n","BG_THRESH_HI=.5\n","BG_THRESH_LO=.1\n","\n","#load an example to void graph problem\n","#TODO fix this.\n","pretrained_model = InceptionResNetV2(include_top=False)\n","img=load_img(\"./ILSVRC2014_train_00010391.JPEG\")\n","x = img_to_array(img)\n","x = np.expand_dims(x, axis=0)\n","not_used=pretrained_model.predict(x)\n","\n","rpn_model = load_model('weights.hdf5',\n","            custom_objects={'loss_cls': loss_cls,'smoothL1':smoothL1})\n","not_used=rpn_model.predict(np.load('n02676566_6914')['fc'])\n","\n","def produce_batch(filepath, gt_boxes, h_w, category):\n","    img=load_img(filepath)\n","    img_width=np.shape(img)[1] * scale[1]\n","    img_height=np.shape(img)[0] * scale[0]\n","    img=img.resize((int(img_width),int(img_height)))\n","    #feed image to pretrained model and get feature map\n","    img = img_to_array(img)\n","    img = np.expand_dims(img, axis=0)\n","    feature_map=pretrained_model.predict(img)\n","    height = np.shape(feature_map)[1]\n","    width = np.shape(feature_map)[2]\n","    num_feature_map=width*height\n","    #calculate output w, h stride\n","    w_stride = h_w[1] / width\n","    h_stride = h_w[0] / height\n","    #generate base anchors according output stride.\n","    #base anchors are 9 anchors wrt a tile (0,0,w_stride-1,h_stride-1)\n","    base_anchors=generate_anchors(w_stride,h_stride)\n","    #slice tiles according to image size and stride.\n","    #each 1x1x1532 feature map is mapping to a tile.\n","    shift_x = np.arange(0, width) * w_stride\n","    shift_y = np.arange(0, height) * h_stride\n","    shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n","    shifts = np.vstack((shift_x.ravel(), shift_y.ravel(), shift_x.ravel(),\n","                            shift_y.ravel())).transpose()\n","    #apply base anchors to all tiles, to have a num_feature_map*9 anchors.\n","    all_anchors = (base_anchors.reshape((1, 9, 4)) +\n","                    shifts.reshape((1, num_feature_map, 4)).transpose((1, 0, 2)))\n","    total_anchors = num_feature_map*9\n","    all_anchors = all_anchors.reshape((total_anchors, 4))\n","    # feed feature map to pretrained RPN model, get proposal labels and bboxes.\n","    res=rpn_model.predict(feature_map)\n","    scores=res[0]\n","    scores=scores.reshape(-1,1)\n","    deltas=res[1]\n","    deltas=np.reshape(deltas,(-1,4))\n","    # proposals transform to bbox values (x1, y1, x2, y2)\n","    proposals =bbox_transform_inv(all_anchors, deltas)\n","    proposals = clip_boxes(proposals, (h_w[0],h_w[1]))\n","    # remove small boxes, here threshold is 40 pixel\n","    keep = filter_boxes(proposals, 40)\n","    proposals = proposals[keep, :]\n","    scores = scores[keep]\n","\n","    # sort socres and only keep top 6000.\n","    pre_nms_topN=6000\n","    order = scores.ravel().argsort()[::-1]\n","    if pre_nms_topN > 0:\n","        order = order[:pre_nms_topN]\n","    proposals = proposals[order, :]\n","    scores = scores[order]\n","    # apply NMS to to 6000, and then keep top 300\n","    post_nms_topN=300\n","    keep = py_cpu_nms(np.hstack((proposals, scores)), 0.7)\n","    if post_nms_topN > 0:\n","        keep = keep[:post_nms_topN]\n","    proposals = proposals[keep, :]\n","    scores = scores[keep]\n","    # add gt_boxes to proposals.\n","    proposals=np.vstack( (proposals, gt_boxes) )\n","    # calculate overlaps of proposal and gt_boxes\n","    overlaps = bbox_overlaps(proposals, gt_boxes)\n","    gt_assignment = overlaps.argmax(axis=1)\n","    max_overlaps = overlaps.max(axis=1)\n","    # labels = gt_labels[gt_assignment] #?\n","\n","    # sub sample\n","    fg_inds = np.where(max_overlaps >= FG_THRESH)[0]\n","    fg_rois_per_this_image = min(int(BATCH*FG_FRAC), fg_inds.size)\n","    # Sample foreground regions without replacement\n","    if fg_inds.size > 0:\n","        fg_inds = npr.choice(fg_inds, size=fg_rois_per_this_image, replace=False)\n","    bg_inds = np.where((max_overlaps < BG_THRESH_HI) &\n","                       (max_overlaps >= BG_THRESH_LO))[0]\n","    bg_rois_per_this_image = BATCH - fg_rois_per_this_image\n","    bg_rois_per_this_image = min(bg_rois_per_this_image, bg_inds.size)\n","    # Sample background regions without replacement\n","    if bg_inds.size > 0:\n","        bg_inds = npr.choice(bg_inds, size=bg_rois_per_this_image, replace=False)\n","    # The indices that we're selecting (both fg and bg)\n","    keep_inds = np.append(fg_inds, bg_inds)\n","    # Select sampled values from various arrays:\n","    # labels = labels[keep_inds]\n","    rois = proposals[keep_inds]\n","    gt_rois=gt_boxes[gt_assignment[keep_inds]]\n","    targets = bbox_transform(rois, gt_rois)#input rois\n","    rois_num=targets.shape[0]\n","    batch_box=np.zeros((rois_num, 200, 4))\n","    for i in range(rois_num):\n","        batch_box[i, category] = targets[i]\n","    batch_box = np.reshape(batch_box, (rois_num, -1))\n","    # get gt category\n","    batch_categories = np.zeros((rois_num, 200, 1))\n","    for i in range(rois_num):\n","        batch_categories[i, category] = 1\n","    batch_categories = np.reshape(batch_categories, (rois_num, -1))\n","    return rois, batch_box, batch_categories\n","\n","##################  generate data  #######################\n","ILSVRC_dataset_path='/home/jk/faster_rcnn/'\n","img_path=ILSVRC_dataset_path+'Data/DET/train/'\n","anno_path=ILSVRC_dataset_path+'Annotations/DET/train/'\n","import glob\n","from multiprocessing import Process, Queue\n","\n","def worker(path):\n","    print('worker start ' + path)\n","    batch_rois=[]\n","    batch_featuremap_inds=[]\n","    batch_categories=[]\n","    batch_bboxes=[]\n","    fc_index=0\n","    dataset={}\n","    #'/ImageSets/DET/train_*'\n","    for fname in glob.glob(ILSVRC_dataset_path+path):\n","        print(fname)\n","        with open(fname,'r') as f:\n","            basename = os.path.basename(fname)\n","            category = int(basename.split('_')[1].split('.')[0])\n","            content=[]\n","            for line in f:\n","                if 'extra' not in line:\n","                    content.append(line)\n","            dataset[category]=content\n","    print(len(dataset))\n","    from random import randint\n","    while 1:\n","        try:\n","            category = randint(1, 200)\n","            content=dataset[category]\n","            n=randint(0,len(content))\n","            line=content[n]\n","            _, gt_boxes, h_w = parse_label(anno_path+line.split()[0]+'.xml')\n","            if len(gt_boxes)==0:\n","                continue\n","            rois, bboxes, categories = produce_batch(img_path+line.split()[0]+'.JPEG', gt_boxes, h_w, category)\n","        except Exception:\n","            # print('parse label or produce batch failed: for: '+line.split()[0])\n","            # traceback.print_exc()\n","            continue\n","        if len(rois) <= 0 :\n","            continue\n","\n","        for i in range(len(rois)):\n","            batch_rois.append(rois[i])\n","            batch_featuremap_inds.append(fc_index)\n","            batch_categories.append(categories[i])\n","            batch_bboxes.append(bboxes[i])\n","        a=feature_map\n","        b=np.asarray(batch_rois)\n","        c=np.asarray(batch_featuremap_inds)\n","        d=np.asarray(batch_categories)\n","        e=np.asarray(batch_bboxes)\n","        f=np.zeros((len(rois),a.shape[1],a.shape[2],a.shape[3]))\n","        f[0]=feature_map[0]\n","        yield [f,b,c], [d,e]\n","        batch_rois=[]\n","        batch_featuremap_inds=[]\n","        batch_categories=[]\n","        batch_bboxes=[]\n","        fc_index=0\n","\n","##################   start train   #######################\n","# model.load_weights('./rcnn_weights_1.hdf5')\n","from keras.callbacks import ModelCheckpoint\n","checkpointer = ModelCheckpoint(filepath='./rcnn_weights_2.hdf5', monitor='loss', verbose=1, save_best_only=True)\n","model.fit_generator(worker('/ImageSets/DET/train_*.txt'), steps_per_epoch=1000, epochs=100, callbacks=[checkpointer])"]},{"cell_type":"code","source":[],"metadata":{"id":"Qo3umnaXXxYD"},"execution_count":null,"outputs":[]}]}